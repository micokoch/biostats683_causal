---
title: "R Assignment 2"
author: "Alvaro J. Castro Rivadeneira"
date: "November 1, 2021"
output: pdf_document
---

```{r echo=F}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
library(tidyverse)
library(ggdag)
library(ggplot2)
library(colorBlindness)
library(here)
```

# 2 Roadmap Questions

\textbf{1. Step 3: Observed data $\&$ link to causal model:} Suppose the observed date consist of $n$ independent, identically distributed (i.i.d.) draws of the random variable $O = (W1, W2, A, Y)$.

(a) Specify the link between the SCM and the observed data.  

We are asked to assume that the observed data $O = (W1, W2, A, Y)$. were generated by sampling $n$ i.i.d. times from a data generating system compatible with $\mathcal{M}^*$. This provides a link between the causal model $\mathcal{M}^*$ and the observed data $O$. The distribution of the background variables $U$ and the structural equations $F$ identify the distribution of the endogenous variables $X$ and thus the distribution of the observed data $O$.  

(b) What restrictions, if any, does the SCM place on the set of allowed distributions for the observed data?  

We have not placed any restrictions on the statistical model $\mathcal{M}$, which is thereby non-parametric.  

(c) What notation do we use to denote the true (but unknown) distribution of the observed data and the statistical model?  

$O = (W1, W2, A, Y) \sim \mathbb{P}_0$

\textbf{2. Step 4-5: Identification $\&$ statistical estimand:}

(a) Using the backdoor criterion, assess identifiability.  

Since we have not made any independence assumptions on the background factors, then there can be no identifiability.  

```{r}
child_mortality <- dagify(y ~ w1 + w2 + a + U,
                          a ~ w1 + w2 + U,
                          w1 ~ U,
                          w2 ~ w1 + U,
                     labels = c("y" = "Child Survival",
                                "a" = "Treatment",
                                "w1" = "Conflict Area",
                                "w2" = "Health Care",
                                "U" = "Unmeasured all"),
                     exposure = "a",
                     outcome = "y",
                     coords = list(x = c(y = 5, a = 2, w1 = 1, w2 = 3, U = 3),
                                   y = c(y = 1, a = 1, w1 = 2, w2 = 2, U = 3))) %>% 
  tidy_dagitty() %>% 
  dplyr::mutate(Variable = case_when(
    name == "y" ~ "Child Survival",
    name == "a" ~ "Treatment",
    name == "w1" ~ "Conflict Area",
    name == "w2" ~ "Health Care",
    name == "U" ~ "Unmeasured (all)"))

child_mortality_dag <- child_mortality %>% 
  ggplot(aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_dag_point(aes(color = Variable)) +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() +
  scale_color_viridis_d()+
  ggtitle("Causal DAG \nEffect of integrated treatment \non child mortality in the Sahel") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(color = guide_legend(override.aes = list(size = 8)))

child_mortality_dag

ggsave(here("r_labs/r_hw2/child_mortality_dag.jpg"), width = 8, height = 6, units = "in")

w_adj_dag_1 <- ggdag_adjustment_set(child_mortality, exposure = "a", outcome = "y", type = "all") +
  theme_dag() +
  labs(title = "Q. 2(a): Assessing identifiability") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

w_adj_dag_1

ggsave(here("r_labs/r_hw2/w_adj_dag_1.jpg"), width = 8, height = 6, units = "in")
```


(b) If the target causal parameter is not identified, under what assumptions would it be?  

We would require some independence assumption between $U_{A}$ and $U_{Y}$, for instance if treatment were randomly assigned. One example is given below, where the target parameter could be identified by adjusting for $W1$ and $W2$:  

```{r}
child_mortality_2 <- dagify(y ~ w1 + w2 + a + Uy,
                          a ~ w1 + w2 + Ua,
                          w1 ~ Uw,
                          w2 ~ w1 + Uw,
                     labels = c("y" = "Child Survival",
                                "a" = "Treatment",
                                "w1" = "Conflict Area",
                                "w2" = "Health Care",
                                "Uw" = "Unmeasured for w1, w2",
                                "Ua" = "Unmeasured a",
                                "Uy" = "Unmeasured y"
                                ),
                     exposure = "a",
                     outcome = "y",
                     coords = list(x = c(y = 5, a = 2, w1 = 1, w2 = 3, Uw = 3, Ua = 2, Uy = 5),
                                   y = c(y = 1, a = 1, w1 = 2, w2 = 2, Uw = 3, Ua = 3, Uy = 3))) %>% 
  tidy_dagitty() %>% 
  dplyr::mutate(Variable = case_when(
    name == "y" ~ "Child Survival",
    name == "a" ~ "Treatment",
    name == "w1" ~ "Conflict Area",
    name == "w2" ~ "Health Care",
    name == "Uw" ~ "Unmeasured (w1, w2)",
    name == "Ua" ~ "Unmeasured (a)",
    name == "Uy" ~ "Unmeasured (y)"))

child_mortality_2_dag <- child_mortality_2 %>% 
  ggplot(aes(
    x = x,
    y = y,
    xend = xend,
    yend = yend
  )) +
  geom_dag_point(aes(color = Variable)) +
  geom_dag_edges() +
  geom_dag_text() +
  theme_dag() +
  scale_color_viridis_d()+
  ggtitle("Causal DAG \nEffect of integrated treatment \non child mortality in the Sahel") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  guides(color = guide_legend(override.aes = list(size = 8)))

child_mortality_2_dag

ggsave(here("r_labs/r_hw2/child_mortality_2_dag.jpg"), width = 8, height = 6, units = "in")

w_adj_dag_2 <- ggdag_adjustment_set(child_mortality_2, exposure = "a", outcome = "y", type = "minimal") +
  theme_dag() +
  labs(title = "Q. 2(a): Assessing identifiability") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

w_adj_dag_2

ggsave(here("r_labs/r_hw2/w_adj_dag_2.jpg"), width = 8, height = 6, units = "in")
```

(c) Specify the target parameter of the observed data distribution (i.e., the statistical estimand). Interpret it.  

We can "commit" to the following interesting statistical estimand, inspired by our scientific/causal question:  
$$
\Psi(\mathbb{P}_0) = \mathbb{E}_0[\mathbb{E}_0(Y|A=1,W1,W2) - \mathbb{E}_0(Y|A=0,W1,W2)]
$$

This means that our target parameter is the expected value of the difference in survival between children given the treatment and those not given the treatment, conitional on the covariates.  

(d) What is the relevant positivity assumption? Is it reasonable here?  

The positivity assumption in this instance is that there are children who will receive treatment and not receive treatment under each of the four covariate permutations - that is, when $W1$ and $W2$ are equal to 0 and 1. In our simplified model, this is a reasonable assumption, if we implement a cluster randomized trial to ensure we have groups under both covariate conditions. Otherwise, it would be very challenging, as it might be impossible to avoid contamination.  

# 3 A specific data generating process

\textbf{1. Evaluate the positivity assumption in closed form for this data generating process.}

$$
\Psi(\mathbb{P}_0) = \mathbb{E}_0[\mathbb{E}_0(Y|A=1,W1,W2) - \mathbb{E}_0(Y|A=0,W1,W2)]
$$
$$
= \sum_{w1,w2}[\mathbb{E}_0(Y|A=1,W1=w1,W2=w2) - \mathbb{E}_0(Y|A=0,W1=w1,W2=w2)]\mathbb{P}_0(W2=w2|W1=w1)\mathbb{P}_0(W1=w1)
$$

In this particular data generating system (one of many compatible with the SCM), the conditional probability of receiving the intervention given the adjustment variables is 

$$
\mathbb{P}_0(A=1|W1,W2) = logit^{-1}(-0.5 + W1 - 1.5*W2)
$$

\textbf{2. \textit{Bonus (Optional):} Evaluate the statistical estimand $\Psi(\mathbb{P_0})$ in closed form for this data generating process.}

```{r}
Psi.P0 <- (plogis(-0.5+0.5-(1.5*0.5)))
Psi.P0
```

# 4 Translate this data generating process into simulations.

\textbf{1. First set the seed to 252.}

```{R s4q1}
set.seed(252)
```

\textbf{2. Write a function to generate the observed data $O = (W1, W2, A, Y)$ and the counterfactual outcomes $(Y_1, Y_0)$.} Recall we generate the counterfactual outcome $Y_1$ by intervening to set the exposure to the combination package $(A=1)$, and we generate the counterfactual outcomes $Y_0$ by intervening to set the exposure to the standard of care $(A=0)$.  Also recall $logit^{-1}$ function is given by the $\texttt{plogis}$ function in $\texttt{R}$.

```{R s4q2}
generate.data <- function(n){
  U.W1<- runif(n, min=0, max=1)
  U.W2<- runif(n, min=0, max=1)
  U.A<- runif(n, min=0, max=1)
  U.Y<- runif(n, min=0, max=1)
  #
  W1 <- as.numeric(U.W1 < 0.5)
  W2 <- as.numeric(U.W2 < 0.5)
  A <- as.numeric(U.A < plogis(-0.5+W1-(1.5*W2)))
  Y <- as.numeric(U.Y < plogis(-0.75+W1-(2*W2)+(2.5*A)+(A*W1)))
  #
  Y.1 <- as.numeric(U.Y < plogis(-0.75+W1-(2*W2)+(2.5)+(W1)))
  Y.0 <- as.numeric(U.Y < plogis(-0.75+W1-(2*W2)))
  #
  data.frame(cbind(W1, W2, A, Y, Y.1, Y.0))
}
```

\textbf{3.  Suppose our target population consists of 100,000 people.  Set the number of draws $n=100,000$.  Use your function to generate $n$ i.i.d. observations.}

```{R s4q3}
n <- 100000
Obs <- generate.data(n)
head(Obs)
summary(Obs)
```

\textbf{4. Does the counterfactual outcome $Y_a$ equal the observed outcome $Y$ when the observed exposure is $A=a$?}

```{R s4q4}
Y.a <- mean(Obs$Y.1) - mean(Obs$Y.0)
Y.a
Y.a1 <- filter(Obs, A==1)
Y.a0 <- filter(Obs, A==0)
Y.obs <- mean(Y.a1$Y.1) - mean(Y.a0$Y.0)
Y.obs
diff <- Y.a - Y.obs
diff
```

No, the observed survival is about 15% higher than the counterfactual.

\textbf{5. \textit{Bonus:} Evaluate and interpret the causal parameter $\Psi^*(\mathbb{P}^*)$}.

# 5 The simple substitution estimator based on the G-Computation formula

\textbf{1.  Set the number of iterations $\texttt{R}$ to 500 and the number of observations $n$ to 200.  Do \textit{not} reset the seed.}

```{R s5q1}
R <- 500
n <- 200
```

\textbf{2.  Create a $R=500$ by 4 matrix $\texttt{estimates}$ to hold the resulting point estimates obtained at each iteration.} The rows will correspond to iterations and the columns to different estimators. 

```{R s5q2}
# Hint: the following code creates a matrix filled with NA of size 10 by 10
# estimates <- matrix(NA, nrow=10, ncol=10)
estimates <- matrix(NA, nrow=R, ncol=4)
```

\textbf{3. Inside a \texttt{for} loop from $\texttt{r}$ equals 1 to $\texttt{R}$ (500), do the following.} \textit{Note: see RAssign2.pdf for further detailed instructions and hints.}

```{R s5q3}
# (a) Use your function from Part 4 to generate n i.i.d. observations. Subset the resulting data frame to only include the observed data O = (W1, W2, A, Y), and name it Obs.

# (b) Copy the data set Obs into two new data frames txt and control.  Then set $A=1 for all units in txt and set $A=0$ for all units in the control.

# (c) Implement the simple substitution estimator (i.e., parametric G-computation) using each one of the four regression specifications above. 

# (d) Assign the resulting point estimates as a row in the matrix estimates.
for(i in 1:R){
  df <- generate.data(n)
  Obs <- subset(df, select=c(W1,W2,A,Y))
  
  reg.model_1 <- glm(Y ~ A, family='binomial', data=Obs)
  reg.model_2 <- glm(Y ~ A + W1, family='binomial', data=Obs)
  reg.model_3 <- glm(Y ~ A + W2, family='binomial', data=Obs)
  reg.model_4 <- glm(Y ~ A*(W1 +W2), family='binomial', data=Obs)
  
  txt<- control <- Obs
  txt$A <- 1
  control$A <- 0
  
  predictY.txt.1 <- predict(reg.model_1, newdata = txt, type='response')
  predictY.txt.2 <- predict(reg.model_2, newdata = txt, type='response')
  predictY.txt.3 <- predict(reg.model_3, newdata = txt, type='response')
  predictY.txt.4 <- predict(reg.model_4, newdata = txt, type='response')
  
  predictY.control.1 <- predict(reg.model_1, newdata = control, type='response')
  predictY.control.2 <- predict(reg.model_2, newdata = control, type='response')
  predictY.control.3 <- predict(reg.model_3, newdata = control, type='response')
  predictY.control.4 <- predict(reg.model_4, newdata = control, type='response')
  
  estimates[i,] <- mean(predictY.txt.1 - predictY.control.1) %>% 
    append(mean(predictY.txt.2 - predictY.control.2)) %>% 
    append(mean(predictY.txt.3 - predictY.control.3)) %>% 
    append(mean(predictY.txt.4 - predictY.control.4))
}
head(estimates)

```

# 6 Performance of the estimators.

\textbf{1. What is the average point estimate from each?}

```{R s6q1}
meanEst.1 <- mean(estimates[,1])
meanEst.2 <- mean(estimates[,2])
meanEst.3 <- mean(estimates[,3])
meanEst.4 <- mean(estimates[,4])
meanEst <- cbind(meanEst.1, meanEst.2, meanEst.3, meanEst.4)
meanEst
estimates %>%
  as_tibble() %>% 
  summarise(across(V1:V4, mean))
```

\textbf{2. Estimate the bias of each estimator.}  For each estimator, average the difference between point estimate $psi_n$ and the truth $psi_0$. 

```{R s6q2}
head(estimates)
bias.mtx <- estimates - Psi.P0
bias <- bias.mtx %>% 
  as_tibble() %>% 
  summarise(across(V1:V4, mean))
bias
```

\textbf{3. Estimate the variance of each estimator.}

```{R s6q3}
var <- estimates %>% 
  as_tibble() %>% 
  summarise(across(V1:V4, var))
var
```

\textbf{4. Estimate the mean squared error of each estimator.}

```{R s6q4}
mse.mtx <- ((estimates-Psi.P0)^2)
mse <- mse.mtx %>% 
  as_tibble() %>% 
  summarise(across(V1:V4, mean))
mse
```

\textbf{5. Briefly comment on the performance of the estimators in this simulation setting. Which estimator has the lowest MSE over the $R=500$ iterations? Are you surprised?}  

The performance improved as the model became more complex, which makes sense. In fact, the fourth model was the only one to include both covariates W1 and W2. The bias and MSE both decreased with model complexity, although the variance increased, which also probably more accurately represents the variability in the data.

# 7 Identifying the mean counterfactual outcome under a dynamic intervention

\textbf{1. Explain why (1) holds using properties of conditional expectations.}  

\newcommand{\indep}{\perp \!\!\! \perp}
Given $Y_d \indep A|W1,W2$ and that W2 is now part of the binary dynamic treatment, the expectation of $Y_d$ is the sum of expectations under each covariate multiplied by the probability of each covariate.  

\textbf{2. Explain why (2) holds using properties of conditional expectations and the fact that $Y_d \perp A|W1, W2$}. Note: No need to explain $Y_d \perp A|W1,W2$ in the context of the study since you have already discussed the assumptions need for the backdoor criterion to hold, and the backdoor criterion implies $Y_d \perp A|W1,W2$.  

The previous expectation can be expanded as is done in (*) by the property of conditional expectations (lecture 6). Given the nature of the dynamic treatment, it can be expanded to be conditional on the assignment $A=d(w2)$. Since it is a dynamic treatment, the distribution should be the same across the assigned treatment.

\textbf{3. Explain why (3) holds.}  

The counterfactual distribution of covariates should be equivalent to the observed distribution of covariates in an RCT.

\textbf{4. Explain why (4) holds.}  

Given the consistency assumption, and the assignment of treatment, the observed distribution of outcomes should be equivalent to the counterfactual distribution.

**Note - I submitted the first version of this assignment on time, but kept updating this file, because there were parts which I hadn't adequately completed. This final version was submitted before the answer key had been made available in the shared drive. Thanks for your understanding.**

